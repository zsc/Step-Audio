# Step-Audio 模块架构与调用关系

## 一、系统架构总览

```
┌─────────────────────────────────────────────────────────┐
│                     用户接口层                            │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │
│  │  app.py  │  │tts_app.py│  │offline   │  │  vLLM   │ │
│  │ (Gradio) │  │ (TTS UI) │  │inference │  │ Server  │ │
│  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │
└─────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────┐
│                     核心业务层                            │
│  ┌────────────────────────────────────────────────────┐ │
│  │               StepAudio (stepaudio.py)              │ │
│  │  整合编码器、LLM、TTS，提供端到端语音对话能力          │ │
│  └────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────┐
│                     模型层                               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │StepAudio     │  │Step-Audio    │  │StepAudioTTS  │  │
│  │Tokenizer     │  │Chat (LLM)    │  │(tts.py)      │  │
│  │(tokenizer.py)│  │(130B Model)  │  │(3B Model)    │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└─────────────────────────────────────────────────────────┘
                            ▼
┌─────────────────────────────────────────────────────────┐
│                     基础组件层                            │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  CosyVoice   │  │   FunASR     │  │   Whisper    │  │
│  │  (声码器)     │  │  (ASR模型)   │  │  (特征提取)   │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└─────────────────────────────────────────────────────────┘
```

## 二、核心模块详解

### 1. StepAudio类 (stepaudio.py)
**职责**: 端到端语音对话的主控制器

**主要属性**:
- `llm_tokenizer`: LLM的文本tokenizer
- `encoder`: 音频编码器(StepAudioTokenizer)
- `decoder`: 语音合成器(StepAudioTTS)
- `llm`: 130B参数的语言模型

**核心方法调用链**:
```python
__call__() 
├── apply_chat_template()  # 构建对话模板
├── encode_audio()         # 如果有音频输入
│   └── encoder.wav2token()
├── llm.generate()         # 生成响应
└── decoder()              # 合成语音
    └── tts.generate()
```

### 2. StepAudioTokenizer类 (tokenizer.py)
**职责**: 音频到离散token的编码

**核心组件**:
- `funasr_model`: Paraformer ASR模型，生成语言级特征
- `ort_session`: ONNX推理会话，生成声学级特征
- `kms`: K-means聚类中心，用于量化

**主要调用流程**:
```python
__call__()
├── wav2token()
│   ├── preprocess_wav()       # 预处理:重采样、能量归一化
│   ├── get_vq02_code()        # 语言级编码
│   │   └── funasr_model.infer_encoder()
│   └── get_vq06_code()        # 声学级编码
│       └── whisper.log_mel_spectrogram()
│       └── ort_session.run()
└── merge_vq0206_to_token_str() # 合并双码本
```

### 3. StepAudioTTS类 (tts.py)
**职责**: 文本到语音的合成

**核心组件**:
- `llm`: 3B参数的TTS语言模型
- `common_cosy_model`: 常规语音的CosyVoice模型
- `music_cosy_model`: 音乐语音的CosyVoice模型

**调用流程**:
```python
__call__()
├── detect_instruction_name()  # 检测指令类型(RAP/哼唱等)
├── preprocess_prompt_wav()    # 处理提示音频(克隆场景)
│   ├── encoder.wav2token()
│   └── cosy_model.frontend._extract_*()
├── tokenize()                 # 构建TTS输入
├── llm.generate()             # 生成音频token
└── cosy_model.token_to_wav_offline() # token转音频
```

## 三、主要调用关系图

### 1. 文本输入场景
```
用户文本输入
    ↓
StepAudio.__call__()
    ↓
apply_chat_template() → 构建prompt
    ↓
llm_tokenizer.encode() → 文本编码
    ↓
llm.generate() → 生成响应token
    ↓
llm_tokenizer.decode() → 解码文本
    ↓
StepAudioTTS.__call__() → 语音合成
    ↓
音频输出
```

### 2. 音频输入场景
```
用户音频输入
    ↓
StepAudio.encode_audio()
    ↓
StepAudioTokenizer.__call__()
    ├── wav2token()
    │   ├── get_vq02_code() [FunASR]
    │   └── get_vq06_code() [Whisper+ONNX]
    └── merge_vq0206_to_token_str()
    ↓
"<audio_xxxxx>" 格式的token序列
    ↓
apply_chat_template() → 嵌入对话模板
    ↓
llm.generate() → 生成响应
    ↓
StepAudioTTS → 语音合成
    ↓
音频输出
```

### 3. 语音克隆场景
```
克隆音频 + 目标文本
    ↓
StepAudioTTS.preprocess_prompt_wav()
    ├── 提取语音特征 (CosyVoice)
    └── 编码音频token (StepAudioTokenizer)
    ↓
构建带音色信息的prompt
    ↓
llm.generate() → 生成个性化音频token
    ↓
CosyVoice.token_to_wav_offline()
    ↓
克隆音色的音频输出
```

## 四、依赖组件

### 1. CosyVoice (cosyvoice/)
- **作用**: 高质量语音合成
- **版本**: 300M参数的常规版和音乐版
- **特点**: 支持Flow Matching和神经声码器

### 2. FunASR (funasr_detach/)
- **作用**: 语音识别和特征提取
- **模型**: Paraformer-large
- **用途**: 生成语言级token (vq02)

### 3. Whisper
- **作用**: 音频特征提取
- **功能**: 计算log-mel频谱图
- **用途**: 为声学级编码提供输入

### 4. Transformers
- **作用**: LLM推理框架
- **功能**: 加载和运行130B/3B参数模型

## 五、关键数据流

### 1. Token编码流
```
音频(16kHz) → FunASR → 语言特征 → K-means量化 → vq02 (16.7Hz)
           ↘ Whisper → Mel频谱 → ONNX模型 → vq06 (25Hz)
                                    ↓
                            2:3交错合并 → 最终token序列
```

### 2. 对话流
```
历史消息 → Chat Template → Prompt构建
                            ↓
                    LLM推理(temperature=0.7)
                            ↓
                    响应token → 文本+音频token分离
                            ↓
                        TTS合成 → 最终音频
```

### 3. 音色控制流
```
提示音频 → 特征提取 ┐
                  ├→ 音色嵌入
目标文本 → 编码    ┘
            ↓
    个性化TTS生成 → 音频输出
```

## 六、性能优化点

1. **流式处理**: StepAudioTokenizer支持流式编码（session管理）
2. **并行推理**: 双码本并行处理提高效率
3. **缓存机制**: vq02编码支持缓存，减少重复计算
4. **张量并行**: vLLM支持的tensor parallel加速130B模型推理
5. **ONNX优化**: 声学编码使用ONNX Runtime加速

## 七、扩展接口

1. **vLLM集成**: call_vllm_chat.py提供高性能推理接口
2. **Gradio界面**: app.py和tts_app.py提供Web交互
3. **离线推理**: offline_inference.py提供批处理能力
4. **Docker部署**: 支持容器化部署，便于扩展